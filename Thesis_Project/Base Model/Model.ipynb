{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acb0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b34e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.6.2\n",
      "TF HUB version:  0.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"TF HUB version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2c1a23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f96270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import stanfordnlp\n",
    "import time\n",
    "import scispacy\n",
    "from tqdm import tqdm\n",
    "from heuristic_sentence_splitter import sent_tokenize_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9589377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6ef4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.sbd_component(doc)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"sbd_component\")\n",
    "def sbd_component(doc):\n",
    "    for i, token in enumerate(doc[:-2]):\n",
    "        # define sentence start if period + titlecase token\n",
    "        if token.text == '.' and doc[i+1].is_title:\n",
    "            doc[i+1].sent_start = True\n",
    "        if token.text == '-' and doc[i+1].text != '-':\n",
    "            doc[i+1].sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load('en_core_sci_md', disable=['tagger','ner', \"lemmatizer\"])\n",
    "nlp.add_pipe(\"sbd_component\", before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7f3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_note(note):\n",
    "    try:\n",
    "        note_text = note['TEXT'] #unicode(note['text'])\n",
    "        note['TEXT'] = ''\n",
    "        processed_sections = process_note_helper(note_text)\n",
    "        ps = {'sections': processed_sections}\n",
    "        ps = pd.DataFrame(ps)\n",
    "        ps.apply(get_sentences, args=(note,), axis=1)\n",
    "        return note \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print ('error', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "576978e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_note_helper(note):\n",
    "    # split note into sections\n",
    "    note_sections = sent_tokenize_rules(note)\n",
    "    processed_sections = []\n",
    "    section_frame = pd.DataFrame({'sections':note_sections})\n",
    "    section_frame.apply(process_section, args=(note,processed_sections,), axis=1)\n",
    "    return(processed_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0268b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_section(section, note, processed_sections):\n",
    "    # perform spacy processing on section\n",
    "    processed_section = nlp(section['sections'])\n",
    "    processed_section = fix_deid_tokens(section['sections'], processed_section)\n",
    "    processed_sections.append(processed_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f5caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert de-identification text into one token\n",
    "def fix_deid_tokens(text, processed_text):\n",
    "    deid_regex  = r\"\\[\\*\\*.{0,15}.*?\\*\\*\\]\" \n",
    "    if text:\n",
    "        indexes = [m.span() for m in re.finditer(deid_regex,text,flags=re.IGNORECASE)]\n",
    "    else:\n",
    "        indexes = []\n",
    "    for start,end in indexes:\n",
    "        head = 0\n",
    "        tail = 0\n",
    "        for token in processed_text:\n",
    "            if start <= token.idx:\n",
    "                head = token.i\n",
    "                break\n",
    "        for token in processed_text:\n",
    "            if end <= token.idx:\n",
    "                tail = token.i\n",
    "                break\n",
    "        if tail == 0:\n",
    "            tail = token.i\n",
    "        with processed_text.retokenize() as retokenizer:\n",
    "            retokenizer.merge(processed_text[head:tail+1])\n",
    "        # processed_text.merge(start_idx=start,end_idx=end)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81840104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(processed_section, note):\n",
    "    # get sentences from spacy processing\n",
    "    sent_frame = pd.DataFrame({'sents': list(processed_section['sections'].sents)})\n",
    "    sent_frame.apply(process_text, args=(note,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35589159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(sent, note):\n",
    "    sent_text = sent['sents'].text\n",
    "    if len(sent_text) > 0 and sent_text.strip() != '\\n':\n",
    "        if '\\n' in sent_text:\n",
    "            sent_text = sent_text.replace('\\n', ' ')\n",
    "        note['TEXT'] += sent_text + '\\n'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cdc2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33cce964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xining\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3072: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./NOTEEVENTS.csv')\n",
    "notes = df.sample(n = 100) # random select 100 rows\n",
    "notes['ind'] = list(range(len(notes.index)))\n",
    "# note_text = notes['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f86e05d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 16.26it/s]\n"
     ]
    }
   ],
   "source": [
    "formatted_notes = notes.progress_apply(process_note, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e51eedf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done formatting notes\n"
     ]
    }
   ],
   "source": [
    "# # save to file\n",
    "# with open('./Discharge Summary 100 subject.txt','w') as f:\n",
    "#     for text in formatted_notes['TEXT']:\n",
    "#         if text != None and len(text) != 0 :\n",
    "#             f.write(text)\n",
    "#             f.write('\\n')\n",
    "# print (\"Done formatting notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78b94759",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Addict\", \n",
    "            \"addiction\",\n",
    "            \"user\", \n",
    "            \"drug abuser\", \n",
    "            \"drug seeking\", \n",
    "            \"abuser\", \n",
    "            \"former addict\", \n",
    "            \"reformed addict\", \n",
    "            \"addicted\", \n",
    "            \"use drugs\", \n",
    "            \"drug baby\", \n",
    "            \"opioid abuse\", \n",
    "            \"opioid dependence\", \n",
    "            \"addiction\", \n",
    "            \"want drugs\", \n",
    "            \"problem\", \n",
    "            \"use problem\", \n",
    "            \"habit\", \n",
    "            \"clean\", \n",
    "            \"clean from drugs\", \n",
    "            \"clean urine test\", \n",
    "            \"dirty urine test\", \n",
    "            \"relapse\", \n",
    "            \"opioid substitution\",\n",
    "            \"relapse therapy\", \n",
    "            \"treatment failure\", \n",
    "            \"being clean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "442f13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = []\n",
    "label_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c52e2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in formatted_notes['TEXT']:\n",
    "    sent = text.split('\\n')\n",
    "    sentence_list += sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33266722",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '|'.join(f\"\\\\b{k}\\\\b\" for k in keywords)  # Whole words only  \n",
    "for sent in sentence_list:\n",
    "    match = re.findall(pattern, sent)\n",
    "    if match:\n",
    "        label_list.append(1)\n",
    "    else:\n",
    "        label_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0078f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3151\n",
      "3151\n"
     ]
    }
   ],
   "source": [
    "# print(len(label_list))\n",
    "# print(len(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5689e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT embedding\n",
    "# load preprocessor and BERT model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1\")\n",
    "encoder_inputs = preprocessor(text_input) # dict with keys: 'input_mask', 'input_type_ids', 'input_word_ids'\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",\n",
    "    trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "089e7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_size = 132\n",
    "head = 0\n",
    "tail = max_batch_size\n",
    "L = len(sentence_list)\n",
    "pooled_embedding = np.zeros((1, 768))\n",
    "sequence_embedding = np.zeros((1, 128, 768))\n",
    "while (tail < L):\n",
    "    batch_sentence_list = sentence_list[head:tail]\n",
    "    encoder_inputs = preprocessor(batch_sentence_list)\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    pooled_output = outputs[\"pooled_output\"]\n",
    "    sequence_output = outputs[\"sequence_output\"]\n",
    "    pooled_embedding = np.append(pooled_embedding, pooled_output, axis=0)\n",
    "    sequence_embedding = np.append(sequence_embedding, sequence_output, axis=0)\n",
    "    head = tail\n",
    "    tail = head + max_batch_size\n",
    "batch_sentence_list = sentence_list[head:]\n",
    "encoder_inputs = preprocessor(batch_sentence_list)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]\n",
    "sequence_output = outputs[\"sequence_output\"]\n",
    "pooled_embedding = np.append(pooled_embedding, pooled_output, axis=0)\n",
    "sequence_embedding = np.append(sequence_embedding, sequence_output, axis=0)\n",
    "pooled_embedding = pooled_embedding[1:]\n",
    "sequence_embedding = sequence_embedding[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f68c3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification using SVM\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7943bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pooled_embedding\n",
    "y = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "13c1084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xining\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Xining\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Xining\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Xining\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Xining\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Xining\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "kf = KFold(n_splits = 10)\n",
    "accuracy_kf = []\n",
    "sensitivity_kf = []\n",
    "specificity_kf = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # svm\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "    TP = confusion[1,1]\n",
    "    TN = confusion[0,0]\n",
    "    FP = confusion[0,1]\n",
    "    FN = confusion[1,0]\n",
    "    accuracy = (TP+TN) / float(TP+TN+FN+FP)\n",
    "    sensitivity = TP / float(TP+FN)\n",
    "    specificity = TN / float(TN+FP)\n",
    "    accuracy_kf.append(accuracy)\n",
    "    sensitivity_kf.append(sensitivity)\n",
    "    specificity_kf.append(specificity)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f9d48845",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_kf = np.array(accuracy_kf)\n",
    "sensitivity_kf = np.array(sensitivity_kf)\n",
    "specificity_kf = np.array(specificity_kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "49be4761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation, acc: 1.00, sensitivity: nan, specificity: 1.00\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(accuracy_kf)\n",
    "sens = np.mean(sensitivity_kf)\n",
    "spec = np.mean(specificity_kf)\n",
    "print('10-fold cross validation, acc: %.2f, sensitivity: %.2f, specificity: %.2f'%(acc, sens, spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8097d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "67bfb2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2083180"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45493019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b48cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
